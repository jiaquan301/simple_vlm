#!/usr/bin/env python3
"""
æœ€ç®€VLMå®ç° - åŸºäºLLMæ‰©å±•çš„è§†è§‰è¯­è¨€æ¨¡å‹
ä½œè€…ï¼šjiaquan301@163.com
åŠŸèƒ½ï¼šåœ¨SimpleLLMåŸºç¡€ä¸Šæ·»åŠ è§†è§‰å¤„ç†èƒ½åŠ›ï¼Œå®ç°å›¾åƒç†è§£å’Œæè¿°ç”Ÿæˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import random
import numpy as np
from PIL import Image
import torchvision.transforms as transforms

# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

class SimpleTokenizer:
    """ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨ï¼ˆä»LLMç»§æ‰¿ï¼‰"""
    def __init__(self, text):
        # è·å–æ‰€æœ‰å”¯ä¸€å­—ç¬¦å¹¶æ’åºï¼Œæ„å»ºè¯æ±‡è¡¨
        self.chars = sorted(list(set(text)))
        self.vocab_size = len(self.chars)
        # å­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„
        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}
        # ç´¢å¼•åˆ°å­—ç¬¦çš„æ˜ å°„
        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}
    
    def encode(self, text):
        """å°†æ–‡æœ¬ç¼–ç ä¸ºtokenç´¢å¼•åˆ—è¡¨"""
        return [self.char_to_idx[ch] for ch in text]
    
    def decode(self, indices):
        """å°†tokenç´¢å¼•åˆ—è¡¨è§£ç ä¸ºæ–‡æœ¬"""
        return ''.join([self.idx_to_char[i] for i in indices])

class SimpleVisionEncoder(nn.Module):
    """
    ç®€åŒ–ç‰ˆè§†è§‰ç¼–ç å™¨ - å°†å›¾åƒè½¬æ¢ä¸ºç‰¹å¾åºåˆ—   
    è¿™æ˜¯VLMçš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€ï¼Œè´Ÿè´£ç†è§£å›¾åƒå†…å®¹
    ç±»ä¼¼äºCLIPçš„å›¾åƒç¼–ç å™¨ï¼Œä½†æ›´ç®€åŒ–
    """
    def __init__(self, image_size=224, patch_size=16, d_model=128, n_layers=2):
        super().__init__()
        self.image_size = image_size
        self.patch_size = patch_size
        self.d_model = d_model
        
        # è®¡ç®—patchæ•°é‡
        self.n_patches = (image_size // patch_size) ** 2
        self.patch_dim = 3 * patch_size * patch_size  # RGB * patch_size^2
        
        print(f"ğŸ–¼ï¸  è§†è§‰ç¼–ç å™¨åˆå§‹åŒ–:")
        print(f"   å›¾åƒå¤§å°: {image_size}x{image_size}")
        print(f"   Patchå¤§å°: {patch_size}x{patch_size}")
        print(f"   Patchæ•°é‡: {self.n_patches}")
        print(f"   è¾“å‡ºç»´åº¦: {d_model}")
        
        # PatchåµŒå…¥å±‚ï¼šå°†å›¾åƒpatchè½¬æ¢ä¸ºå‘é‡
        self.patch_embedding = nn.Linear(self.patch_dim, d_model)
        
        # ä½ç½®åµŒå…¥ï¼šä¸ºæ¯ä¸ªpatchæ·»åŠ ä½ç½®ä¿¡æ¯
        self.position_embedding = nn.Parameter(torch.randn(1, self.n_patches, d_model))
        
        # ç®€åŒ–çš„Transformerç¼–ç å™¨
        self.transformer_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=4,
                dim_feedforward=d_model * 4,
                dropout=0.1,
                batch_first=True
            ) for _ in range(n_layers)
        ])
        
        # å±‚å½’ä¸€åŒ–
        self.norm = nn.LayerNorm(d_model)
        
    def patchify(self, images):
        """
        å°†å›¾åƒåˆ†å‰²ä¸ºpatches
        è¾“å…¥: (batch_size, 3, H, W)
        è¾“å‡º: (batch_size, n_patches, patch_dim)
        """
        batch_size, channels, height, width = images.shape
        
        # ç¡®ä¿å›¾åƒå°ºå¯¸æ­£ç¡®
        assert height == width == self.image_size, f"å›¾åƒå°ºå¯¸åº”ä¸º{self.image_size}x{self.image_size}"
        
        # åˆ†å‰²ä¸ºpatches
        patches = images.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)
        patches = patches.contiguous().view(batch_size, channels, -1, self.patch_size, self.patch_size)
        patches = patches.permute(0, 2, 1, 3, 4).contiguous()
        patches = patches.view(batch_size, self.n_patches, -1)
        
        return patches
    
    def forward(self, images):
        """
        å‰å‘ä¼ æ’­
        è¾“å…¥: (batch_size, 3, H, W) å›¾åƒå¼ é‡
        è¾“å‡º: (batch_size, n_patches, d_model) è§†è§‰ç‰¹å¾
        """
        batch_size = images.shape[0]
        
        # 1. å°†å›¾åƒåˆ†å‰²ä¸ºpatches
        patches = self.patchify(images)  # (batch_size, n_patches, patch_dim)
        
        # 2. PatchåµŒå…¥
        x = self.patch_embedding(patches)  # (batch_size, n_patches, d_model)
        
        # 3. æ·»åŠ ä½ç½®åµŒå…¥
        x = x + self.position_embedding
        
        # 4. é€šè¿‡Transformerå±‚
        for layer in self.transformer_layers:
            x = layer(x)
        
        # 5. å±‚å½’ä¸€åŒ–
        x = self.norm(x)
        
        return x

class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆä»LLMç»§æ‰¿ï¼‰"""
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_linear = nn.Linear(d_model, d_model)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape
        
        Q = self.q_linear(x)
        K = self.k_linear(x)
        V = self.v_linear(x)
        
        Q = Q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        if mask is not None:
            scores.masked_fill_(mask, float('-inf'))
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_output = torch.matmul(attention_weights, V)
        
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model)
        
        return self.out_linear(attention_output)

class CrossModalAttention(nn.Module):
    """
    è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ - VLMçš„æ ¸å¿ƒåˆ›æ–°
    
    å…è®¸æ–‡æœ¬tokenå…³æ³¨å›¾åƒç‰¹å¾ï¼Œå®ç°è§†è§‰-è¯­è¨€äº¤äº’
    è¿™æ˜¯VLMç›¸æ¯”LLMçš„å…³é”®æ‰©å±•
    """
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        
        # Queryæ¥è‡ªæ–‡æœ¬ï¼ŒKeyå’ŒValueæ¥è‡ªå›¾åƒ
        self.q_linear = nn.Linear(d_model, d_model)  # æ–‡æœ¬æŸ¥è¯¢
        self.k_linear = nn.Linear(d_model, d_model)  # å›¾åƒé”®
        self.v_linear = nn.Linear(d_model, d_model)  # å›¾åƒå€¼
        self.out_linear = nn.Linear(d_model, d_model)
        
    def forward(self, text_features, image_features):
        """
        è·¨æ¨¡æ€æ³¨æ„åŠ›è®¡ç®—
        text_features: (batch_size, text_len, d_model) æ–‡æœ¬ç‰¹å¾
        image_features: (batch_size, image_len, d_model) å›¾åƒç‰¹å¾
        """
        batch_size, text_len, d_model = text_features.shape
        _, image_len, _ = image_features.shape
        
        # æ–‡æœ¬ä½œä¸ºQuery
        Q = self.q_linear(text_features)
        # å›¾åƒä½œä¸ºKeyå’ŒValue
        K = self.k_linear(image_features)
        V = self.v_linear(image_features)
        
        # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        Q = Q.view(batch_size, text_len, self.n_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, image_len, self.n_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, image_len, self.n_heads, self.head_dim).transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼šæ–‡æœ¬å…³æ³¨å›¾åƒ
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # åº”ç”¨softmax
        attention_weights = F.softmax(scores, dim=-1)
        
        # åŠ æƒæ±‚å’Œå›¾åƒç‰¹å¾
        attention_output = torch.matmul(attention_weights, V)
        
        # é‡å¡‘å¹¶è¾“å‡º
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, text_len, d_model)
        
        return self.out_linear(attention_output)

class VLMTransformerBlock(nn.Module):
    """
    VLMä¸“ç”¨çš„Transformerå—
    
    åœ¨æ ‡å‡†Transformerå—åŸºç¡€ä¸Šæ·»åŠ è·¨æ¨¡æ€æ³¨æ„åŠ›
    è¿™æ˜¯VLMèƒ½å¤Ÿç†è§£å›¾åƒå’Œæ–‡æœ¬å…³ç³»çš„å…³é”®
    """
    def __init__(self, d_model, n_heads, d_ff):
        super().__init__()
        
        # è‡ªæ³¨æ„åŠ›ï¼ˆæ–‡æœ¬å†…éƒ¨ï¼‰
        self.self_attention = MultiHeadAttention(d_model, n_heads)
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆæ–‡æœ¬å…³æ³¨å›¾åƒï¼‰
        self.cross_attention = CrossModalAttention(d_model, n_heads)
        
        # å‰é¦ˆç½‘ç»œ
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        
        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
    def forward(self, text_features, image_features, causal_mask=None):
        """
        å‰å‘ä¼ æ’­
        text_features: æ–‡æœ¬ç‰¹å¾
        image_features: å›¾åƒç‰¹å¾
        causal_mask: å› æœæ©ç ï¼ˆç”¨äºæ–‡æœ¬ç”Ÿæˆï¼‰
        """
        # 1. æ–‡æœ¬è‡ªæ³¨æ„åŠ›
        attn_output = self.self_attention(text_features, causal_mask)
        text_features = self.norm1(text_features + attn_output)
        
        # 2. è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆæ–‡æœ¬å…³æ³¨å›¾åƒï¼‰
        cross_attn_output = self.cross_attention(text_features, image_features)
        text_features = self.norm2(text_features + cross_attn_output)
        
        # 3. å‰é¦ˆç½‘ç»œ
        ff_output = self.feed_forward(text_features)
        text_features = self.norm3(text_features + ff_output)
        
        return text_features

class SimpleVLM(nn.Module):
    """
    ç®€åŒ–ç‰ˆè§†è§‰è¯­è¨€æ¨¡å‹
    
    åœ¨SimpleLLMåŸºç¡€ä¸Šæ·»åŠ è§†è§‰å¤„ç†èƒ½åŠ›
    æ ¸å¿ƒæ€æƒ³ï¼šå›¾åƒ + æ–‡æœ¬ â†’ ç†è§£ â†’ ç”Ÿæˆ
    """
    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=2, max_seq_len=64, 
                 image_size=224, patch_size=16):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        
        print(f"ğŸ¤– åˆå§‹åŒ–SimpleVLM:")
        print(f"   è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"   æ¨¡å‹ç»´åº¦: {d_model}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
        print(f"   Transformerå±‚æ•°: {n_layers}")
        
        # 1. è§†è§‰ç¼–ç å™¨ï¼šå¤„ç†å›¾åƒ
        self.vision_encoder = SimpleVisionEncoder(
            image_size=image_size, 
            patch_size=patch_size, 
            d_model=d_model
        )
        
        # 2. æ–‡æœ¬åµŒå…¥å±‚ï¼šå¤„ç†æ–‡æœ¬ï¼ˆä»LLMç»§æ‰¿ï¼‰
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_len, d_model)
        
        # 3. VLMä¸“ç”¨Transformerå—
        self.vlm_blocks = nn.ModuleList([
            VLMTransformerBlock(d_model, n_heads, d_model * 4) 
            for _ in range(n_layers)
        ])
        
        # 4. è¾“å‡ºå±‚ï¼šç”Ÿæˆæ–‡æœ¬
        self.output_projection = nn.Linear(d_model, vocab_size)
        
        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
        
    def forward(self, images, text_tokens):
        """
        å‰å‘ä¼ æ’­
        images: (batch_size, 3, H, W) å›¾åƒå¼ é‡
        text_tokens: (batch_size, seq_len) æ–‡æœ¬token
        """
        batch_size, seq_len = text_tokens.shape
        
        # 1. å¤„ç†å›¾åƒï¼šæå–è§†è§‰ç‰¹å¾
        image_features = self.vision_encoder(images)  # (batch_size, n_patches, d_model)
        
        # 2. å¤„ç†æ–‡æœ¬ï¼šåµŒå…¥å’Œä½ç½®ç¼–ç 
        positions = torch.arange(seq_len, device=text_tokens.device).unsqueeze(0).expand(batch_size, -1)
        text_features = self.token_embedding(text_tokens) + self.position_embedding(positions)
        
        # 3. åˆ›å»ºå› æœæ©ç ï¼ˆç¡®ä¿ç”Ÿæˆæ—¶ä¸èƒ½çœ‹åˆ°æœªæ¥tokenï¼‰
        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # æ‰©å±•ç»´åº¦åŒ¹é…å¤šå¤´æ³¨æ„åŠ›
        
        # 4. é€šè¿‡VLM Transformerå—ï¼šèåˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯
        for vlm_block in self.vlm_blocks:
            text_features = vlm_block(text_features, image_features, causal_mask)
        
        # 5. è¾“å‡ºæŠ•å½±ï¼šç”Ÿæˆè¯æ±‡è¡¨æ¦‚ç‡
        logits = self.output_projection(text_features)
        
        return logits
    
    def generate_caption(self, image, tokenizer, max_length=50, temperature=1.0):
        """
        å›¾åƒæè¿°ç”Ÿæˆ
        
        è¿™æ˜¯VLMçš„æ ¸å¿ƒåº”ç”¨ï¼šçœ‹å›¾è¯´è¯
        è¾“å…¥å›¾åƒï¼Œè¾“å‡ºæ–‡å­—æè¿°
        """
        self.eval()
        
        # å‡†å¤‡å›¾åƒï¼ˆæ·»åŠ batchç»´åº¦ï¼‰
        if len(image.shape) == 3:
            image = image.unsqueeze(0)
        
        # åˆå§‹åŒ–ç”Ÿæˆåºåˆ—ï¼ˆä»ç‰¹æ®Šå¼€å§‹tokenå¼€å§‹ï¼‰
        generated_tokens = [0]  # å‡è®¾0æ˜¯å¼€å§‹token
        
        print(f"ğŸ¯ å¼€å§‹ç”Ÿæˆå›¾åƒæè¿°...")
        
        with torch.no_grad():
            for step in range(max_length):
                # å‡†å¤‡å½“å‰åºåˆ—
                current_tokens = torch.tensor([generated_tokens])
                
                # ç¡®ä¿åºåˆ—é•¿åº¦ä¸è¶…è¿‡æœ€å¤§é•¿åº¦
                if len(generated_tokens) >= self.max_seq_len:
                    current_tokens = torch.tensor([generated_tokens[-self.max_seq_len:]])
                
                # å‰å‘ä¼ æ’­
                logits = self.forward(image, current_tokens)
                
                # è·å–æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹
                next_token_logits = logits[0, -1, :] / temperature
                
                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, 1).item()
                
                generated_tokens.append(next_token)
                
                # å¦‚æœç”Ÿæˆäº†ç»“æŸtokenï¼Œåœæ­¢ç”Ÿæˆ
                if next_token == 0:  # å‡è®¾0ä¹Ÿæ˜¯ç»“æŸtoken
                    break
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        try:
            caption = tokenizer.decode(generated_tokens[1:])  # è·³è¿‡å¼€å§‹token
            print(f"âœ… ç”Ÿæˆå®Œæˆ: '{caption}'")
            return caption
        except:
            print("âš ï¸  è§£ç å¤±è´¥ï¼Œè¿”å›åŸå§‹token")
            return str(generated_tokens)

def create_dummy_image(size=(224, 224)):
    """
    åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿå›¾åƒç”¨äºæ¼”ç¤º
    
    åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯çœŸå®çš„å›¾åƒæ•°æ®
    """
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„å½©è‰²å›¾åƒ
    image = np.random.rand(3, size[0], size[1]).astype(np.float32)
    
    # æ·»åŠ ä¸€äº›ç»“æ„åŒ–çš„æ¨¡å¼ï¼Œè®©å›¾åƒæ›´æœ‰æ„ä¹‰
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„"ç‰©ä½“"åœ¨å›¾åƒä¸­å¿ƒ
    center_x, center_y = size[0] // 2, size[1] // 2
    for i in range(center_x - 20, center_x + 20):
        for j in range(center_y - 20, center_y + 20):
            if 0 <= i < size[0] and 0 <= j < size[1]:
                image[:, i, j] = [0.8, 0.2, 0.2]  # çº¢è‰²æ–¹å—
    
    return torch.tensor(image)

def train_simple_vlm():
    """è®­ç»ƒç®€å•VLMçš„ç¤ºä¾‹"""
    print("ğŸš€ å¼€å§‹VLMè®­ç»ƒæ¼”ç¤º")
    print("="*60)
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    text = """
    è¿™æ˜¯ä¸€ä¸ªçº¢è‰²çš„æ–¹å—ã€‚å›¾åƒä¸­å¤®æœ‰ä¸€ä¸ªçº¢è‰²ç‰©ä½“ã€‚
    çº¢è‰²æ–¹å—ä½äºå›¾åƒä¸­å¿ƒã€‚è¿™ä¸ªç‰©ä½“æ˜¯çº¢è‰²çš„ã€‚
    å›¾åƒæ˜¾ç¤ºäº†ä¸€ä¸ªçº¢è‰²çš„æ­£æ–¹å½¢ã€‚ä¸­å¤®æ˜¯çº¢è‰²åŒºåŸŸã€‚
    """
    
    print(f"ğŸ“š è®­ç»ƒæ–‡æœ¬: {text[:50]}...")
    
    # åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹
    tokenizer = SimpleTokenizer(text)
    model = SimpleVLM(vocab_size=tokenizer.vocab_size, d_model=64, n_heads=4, n_layers=2)
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    input_ids = tokenizer.encode(text)
    
    # åˆ›å»ºè™šæ‹Ÿå›¾åƒæ•°æ®
    dummy_image = create_dummy_image()
    
    # è®­ç»ƒè®¾ç½®
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    model.train()
    
    print(f"\nğŸ‹ï¸  å¼€å§‹è®­ç»ƒ...")
    
    for epoch in range(50):  # å‡å°‘è®­ç»ƒè½®æ•°ï¼Œå› ä¸ºVLMæ›´å¤æ‚
        # éšæœºé€‰æ‹©æ–‡æœ¬ç‰‡æ®µ
        start_idx = random.randint(0, max(0, len(input_ids) - model.max_seq_len - 1))
        end_idx = start_idx + min(model.max_seq_len, len(input_ids) - start_idx - 1)
        
        # è¾“å…¥å’Œç›®æ ‡
        x_text = torch.tensor([input_ids[start_idx:end_idx]])
        y_text = torch.tensor([input_ids[start_idx+1:end_idx+1]])
        
        # å›¾åƒè¾“å…¥ï¼ˆæ‰¹æ¬¡ç»´åº¦ï¼‰
        x_image = dummy_image.unsqueeze(0)
        
        # å‰å‘ä¼ æ’­
        logits = model(x_image, x_text)
        
        # è®¡ç®—æŸå¤±
        loss = F.cross_entropy(logits.view(-1, tokenizer.vocab_size), y_text.view(-1))
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if epoch % 10 == 0:
            print(f"   Epoch {epoch:2d}, Loss: {loss.item():.4f}")
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    
    # æµ‹è¯•å›¾åƒæè¿°ç”Ÿæˆ
    print(f"\nğŸ¯ æµ‹è¯•å›¾åƒæè¿°ç”Ÿæˆ:")
    print("-" * 40)
    
    test_image = create_dummy_image()
    caption = model.generate_caption(test_image, tokenizer, max_length=20, temperature=0.8)
    
    print(f"ğŸ–¼ï¸  è¾“å…¥: è™šæ‹Ÿçº¢è‰²æ–¹å—å›¾åƒ")
    print(f"ğŸ“ ç”Ÿæˆæè¿°: {caption}")
    
    return model, tokenizer

def demonstrate_vlm_components():
    """æ¼”ç¤ºVLMå„ä¸ªç»„ä»¶çš„å·¥ä½œåŸç†"""
    print("\n" + "="*60)
    print("ğŸ” VLMç»„ä»¶æ¼”ç¤º")
    print("="*60)
    
    # 1. è§†è§‰ç¼–ç å™¨æ¼”ç¤º
    print("\n1ï¸âƒ£ è§†è§‰ç¼–ç å™¨æ¼”ç¤º:")
    vision_encoder = SimpleVisionEncoder(image_size=224, patch_size=16, d_model=64)
    
    dummy_image = create_dummy_image().unsqueeze(0)  # æ·»åŠ batchç»´åº¦
    visual_features = vision_encoder(dummy_image)
    
    print(f"   è¾“å…¥å›¾åƒå½¢çŠ¶: {dummy_image.shape}")
    print(f"   è¾“å‡ºç‰¹å¾å½¢çŠ¶: {visual_features.shape}")
    print(f"   ç‰¹å¾å«ä¹‰: {visual_features.shape[1]}ä¸ªå›¾åƒpatchï¼Œæ¯ä¸ªpatchç”¨{visual_features.shape[2]}ç»´å‘é‡è¡¨ç¤º")
    
    # 2. è·¨æ¨¡æ€æ³¨æ„åŠ›æ¼”ç¤º
    print("\n2ï¸âƒ£ è·¨æ¨¡æ€æ³¨æ„åŠ›æ¼”ç¤º:")
    cross_attention = CrossModalAttention(d_model=64, n_heads=4)
    
    # æ¨¡æ‹Ÿæ–‡æœ¬ç‰¹å¾
    text_features = torch.randn(1, 10, 64)  # 10ä¸ªæ–‡æœ¬token
    
    # è®¡ç®—è·¨æ¨¡æ€æ³¨æ„åŠ›
    attended_features = cross_attention(text_features, visual_features)
    
    print(f"   æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {text_features.shape}")
    print(f"   å›¾åƒç‰¹å¾å½¢çŠ¶: {visual_features.shape}")
    print(f"   æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {attended_features.shape}")
    print(f"   å«ä¹‰: æ–‡æœ¬tokenå…³æ³¨å›¾åƒpatchï¼Œè·å¾—è§†è§‰ä¿¡æ¯")

if __name__ == "__main__":
    print("ğŸ¨ æœ€ç®€VLMå®ç°æ¼”ç¤º")
    print("="*60)
    print("è¿™æ˜¯ä¸€ä¸ªåŸºäºSimpleLLMæ‰©å±•çš„è§†è§‰è¯­è¨€æ¨¡å‹")
    print("æ ¸å¿ƒåŠŸèƒ½ï¼šå›¾åƒç†è§£ + æ–‡æœ¬ç”Ÿæˆ = çœ‹å›¾è¯´è¯")
    print()
    
    # æ¼”ç¤ºVLMç»„ä»¶
    demonstrate_vlm_components()
    
    # è®­ç»ƒå’Œæµ‹è¯•VLM
    model, tokenizer = train_simple_vlm()
    
    print("\n" + "="*60)
    print("ğŸ‰ VLMæ¼”ç¤ºå®Œæˆï¼")
    print("="*60)
    print("é€šè¿‡è¿™ä¸ªæ¼”ç¤ºï¼Œä½ äº†è§£äº†:")
    print("âœ… å¦‚ä½•ä»LLMæ‰©å±•åˆ°VLM")
    print("âœ… è§†è§‰ç¼–ç å™¨çš„å·¥ä½œåŸç†")
    print("âœ… è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶")
    print("âœ… å›¾åƒæè¿°ç”Ÿæˆè¿‡ç¨‹")
    print("\nğŸ’¡ VLMçš„æ ¸å¿ƒæ€æƒ³:")
    print("   å›¾åƒç¼–ç å™¨æå–è§†è§‰ç‰¹å¾")
    print("   è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯")
    print("   è¯­è¨€æ¨¡å‹ç”Ÿæˆæè¿°æ–‡æœ¬")

