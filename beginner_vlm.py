#!/usr/bin/env python3
"""
é›¶åŸºç¡€VLMæ•™ç¨‹ - é…å¥—æ¼”ç¤ºä»£ç ï¼ˆä¸Žå…¥é—¨åšå®¢ä»£ç ä¸€è‡´ç‰ˆæœ¬ï¼‰
========================================================

è¿™ä¸ªæ–‡ä»¶åŸºäºŽé›¶åŸºç¡€å…¥é—¨åšå®¢ä¸­çš„VLMä»£ç ï¼Œæ·»åŠ äº†è¯¦ç»†çš„è§£é‡Šã€‚
ç¡®ä¿ä»£ç å®Œå…¨ä¸€è‡´ï¼Œåªæ˜¯å¢žåŠ äº†æ›´å¤šçš„æ³¨é‡Šå’Œæ¼”ç¤ºåŠŸèƒ½ã€‚

è¿è¡Œè¿™ä¸ªæ–‡ä»¶ï¼Œä½ å°†çœ‹åˆ°ï¼š
1. ä¸Žå…¥é—¨åšå®¢å®Œå…¨ä¸€è‡´çš„VLMå®žçŽ°
2. æ¯ä¸ªç»„ä»¶çš„è¯¦ç»†å·¥ä½œåŽŸç†è§£é‡Š
3. è®­ç»ƒè¿‡ç¨‹çš„å¯è§†åŒ–åˆ†æž
4. æ¨¡åž‹è¡Œä¸ºçš„æ·±å…¥ç†è§£
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import random
import numpy as np
from PIL import Image
import torchvision.transforms as transforms

# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æžœå¯å¤çŽ°
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

print("ðŸš€ é›¶åŸºç¡€VLMæ•™ç¨‹")
print("=" * 60)
print("æœ¬æ¼”ç¤ºä½¿ç”¨ä¸Žåšå®¢å®Œå…¨ç›¸åŒçš„ä»£ç æž¶æž„")
print("åªæ˜¯æ·»åŠ äº†æ›´è¯¦ç»†çš„è§£é‡Šå’Œæ¼”ç¤ºåŠŸèƒ½")
print("=" * 60)

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç»„ä»¶
# ============================================================================

class SimpleTokenizer:
    """
    ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨
    
    ðŸ”¤ ä½œç”¨ï¼šæŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—ï¼Œè®©è®¡ç®—æœºèƒ½ç†è§£
    ðŸ“ ä¾‹å­ï¼š'çº¢è‰²æ–¹å—' â†’ [1, 2, 3, 4]
    """
    def __init__(self, text):
        print("ðŸ“š åˆå§‹åŒ–åˆ†è¯å™¨...")
        
        # èŽ·å–æ‰€æœ‰å”¯ä¸€å­—ç¬¦å¹¶æŽ’åºï¼Œæž„å»ºè¯æ±‡è¡¨
        self.chars = sorted(list(set(text)))
        self.vocab_size = len(self.chars)
        
        # å­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„
        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}
        # ç´¢å¼•åˆ°å­—ç¬¦çš„æ˜ å°„
        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}
        
        print(f"   è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        print(f"   è¯æ±‡è¡¨å†…å®¹: {self.chars[:10]}..." if len(self.chars) > 10 else f"   è¯æ±‡è¡¨å†…å®¹: {self.chars}")
    
    def encode(self, text):
        """å°†æ–‡æœ¬ç¼–ç ä¸ºtokenç´¢å¼•åˆ—è¡¨"""
        return [self.char_to_idx[ch] for ch in text]
    
    def decode(self, indices):
        """å°†tokenç´¢å¼•åˆ—è¡¨è§£ç ä¸ºæ–‡æœ¬"""
        return ''.join([self.idx_to_char[i] for i in indices])

class SimpleVisionEncoder(nn.Module):
    """
    ç®€åŒ–ç‰ˆè§†è§‰ç¼–ç å™¨ - VLMçš„"çœ¼ç›"
    
    ðŸŽ¯ ä½œç”¨ï¼šå°†å›¾åƒè½¬æ¢ä¸ºç‰¹å¾åºåˆ—ï¼Œè®©è®¡ç®—æœºèƒ½"çœ‹æ‡‚"å›¾ç‰‡
    ðŸ” åŽŸç†ï¼š
    1. æŠŠå›¾ç‰‡åˆ‡æˆå°å—ï¼ˆåƒæ‹¼å›¾ä¸€æ ·ï¼‰
    2. æ¯ä¸ªå°å—è½¬æ¢æˆç‰¹å¾å‘é‡
    3. æ·»åŠ ä½ç½®ä¿¡æ¯
    4. ç”¨Transformerå¤„ç†ç‰¹å¾å…³ç³»
    
    ðŸ“Š è¾“å…¥ï¼šå›¾åƒ (3, 224, 224)
    ðŸ“Š è¾“å‡ºï¼šç‰¹å¾åºåˆ— (196, d_model)
    """
    def __init__(self, image_size=224, patch_size=16, d_model=128, n_layers=2):
        super().__init__()
        print(f"ðŸ‘ï¸ åˆå§‹åŒ–è§†è§‰ç¼–ç å™¨...")
        
        self.image_size = image_size
        self.patch_size = patch_size
        self.d_model = d_model
        
        # è®¡ç®—å›¾ç‰‡ä¼šè¢«åˆ†æˆå¤šå°‘ä¸ªå°å—
        self.num_patches = (image_size // patch_size) ** 2  # 14Ã—14 = 196ä¸ªå°å—
        self.patch_dim = patch_size * patch_size * 3  # æ¯ä¸ªå°å—çš„åƒç´ æ•°ï¼š16Ã—16Ã—3 = 768
        
        print(f"   å›¾ç‰‡å¤§å°: {image_size}Ã—{image_size}")
        print(f"   åˆ†å—å¤§å°: {patch_size}Ã—{patch_size}")
        print(f"   æ€»å—æ•°: {self.num_patches}")
        print(f"   æ¯å—åƒç´ æ•°: {self.patch_dim}")
        
        # çº¿æ€§å±‚ï¼šæŠŠå›¾ç‰‡å°å—è½¬æ¢æˆç‰¹å¾å‘é‡
        # å°±åƒæŠŠ768ä¸ªåƒç´ å€¼åŽ‹ç¼©æˆ128ä¸ªç‰¹å¾å€¼
        self.patch_embedding = nn.Linear(self.patch_dim, d_model)
        
        # ä½ç½®ç¼–ç ï¼šå‘Šè¯‰æ¨¡åž‹æ¯ä¸ªå°å—åœ¨å›¾ç‰‡ä¸­çš„ä½ç½®
        # å°±åƒç»™æ‹¼å›¾å—è´´æ ‡ç­¾ï¼š"æˆ‘æ˜¯ç¬¬3è¡Œç¬¬5åˆ—çš„é‚£ä¸€å—"
        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches, d_model))
        
        # Transformerç¼–ç å™¨ï¼šè®©å°å—ä¹‹é—´èƒ½å¤Ÿ"äº¤æµ"
        # è¿™æ ·æ¨¡åž‹èƒ½ç†è§£"å·¦è¾¹çš„çº¢è‰²å°å—å’Œå³è¾¹çš„è“è‰²å°å—ç»„æˆäº†ä¸€ä¸ªç‰©ä½“"
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=8, 
            dim_feedforward=d_model*4,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        
        print(f"   ç‰¹å¾ç»´åº¦: {d_model}")
        print(f"   Transformerå±‚æ•°: {n_layers}")
    
    def forward(self, x):
        """
        å¤„ç†å›¾åƒçš„å®Œæ•´æµç¨‹
        
        ðŸ”„ å¤„ç†æ­¥éª¤ï¼š
        1. å›¾ç‰‡åˆ†å—ï¼š(B, 3, 224, 224) â†’ (B, 196, 768)
        2. ç‰¹å¾æ˜ å°„ï¼š(B, 196, 768) â†’ (B, 196, 128)
        3. ä½ç½®ç¼–ç ï¼šæ·»åŠ ä½ç½®ä¿¡æ¯
        4. Transformerï¼šç†è§£å—ä¹‹é—´å…³ç³»
        """
        B = x.shape[0]  # batch size
        
        # ç¬¬ä¸€æ­¥ï¼šæŠŠå›¾ç‰‡åˆ†æˆå°å—
        # unfoldæ“ä½œå°±åƒç”¨åˆ€æŠŠå›¾ç‰‡åˆ‡æˆè§„æ•´çš„å°æ–¹å—
        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)
        x = x.contiguous().view(B, 3, -1, self.patch_size, self.patch_size)
        x = x.permute(0, 2, 1, 3, 4).contiguous()
        x = x.view(B, self.num_patches, -1)  # (B, 196, 768)
        
        # ç¬¬äºŒæ­¥ï¼šæ¯ä¸ªå°å—è½¬æ¢æˆç‰¹å¾å‘é‡
        # å°±åƒæŠŠæ¯ä¸ªæ‹¼å›¾å—çš„é¢œè‰²ã€çº¹ç†ä¿¡æ¯æå–å‡ºæ¥
        x = self.patch_embedding(x)  # (B, 196, 128)
        
        # ç¬¬ä¸‰æ­¥ï¼šæ·»åŠ ä½ç½®ç¼–ç 
        # å‘Šè¯‰æ¨¡åž‹æ¯ä¸ªç‰¹å¾æ¥è‡ªå›¾ç‰‡çš„å“ªä¸ªä½ç½®
        x = x + self.position_embedding
        
        # ç¬¬å››æ­¥ï¼šTransformerå¤„ç†
        # è®©ä¸åŒä½ç½®çš„ç‰¹å¾èƒ½å¤Ÿ"äº¤æµ"ï¼Œç†è§£æ•´ä½“å›¾åƒ
        x = self.transformer(x)  # (B, 196, 128)
        
        return x

class CrossModalAttention(nn.Module):
    """
    è·¨æ¨¡æ€æ³¨æ„åŠ› - è®©æ–‡å­—å’Œå›¾ç‰‡"å¯¹è¯"çš„æ¡¥æ¢
    
    ðŸŒ‰ ä½œç”¨ï¼šå»ºç«‹æ–‡å­—å’Œå›¾ç‰‡ä¹‹é—´çš„å…³è”
    ðŸ’­ åŽŸç†ï¼š
    - æ–‡å­—ä½œä¸º"é—®é¢˜"ï¼ˆQueryï¼‰ï¼šæˆ‘æƒ³äº†è§£ä»€ä¹ˆï¼Ÿ
    - å›¾ç‰‡ä½œä¸º"ç­”æ¡ˆåº“"ï¼ˆKeyå’ŒValueï¼‰ï¼šæˆ‘èƒ½æä¾›ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
    - æ ¹æ®ç›¸å…³æ€§é€‰æ‹©æœ€é‡è¦çš„å›¾ç‰‡åŒºåŸŸ
    
    ðŸŽ¯ ä¾‹å­ï¼š
    - ç”Ÿæˆ"çº¢è‰²"æ—¶ â†’ ä¸»è¦å…³æ³¨å›¾ç‰‡ä¸­çš„çº¢è‰²åŒºåŸŸ
    - ç”Ÿæˆ"æ–¹å—"æ—¶ â†’ ä¸»è¦å…³æ³¨å›¾ç‰‡ä¸­çš„æ–¹å½¢è¾¹ç¼˜
    """
    def __init__(self, d_model, n_heads=8):
        super().__init__()
        print(f"ðŸ”— åˆå§‹åŒ–è·¨æ¨¡æ€æ³¨æ„åŠ›...")
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        
        # å¤šå¤´æ³¨æ„åŠ›ï¼šåŒæ—¶ä»Žå¤šä¸ªè§’åº¦ç†è§£å›¾æ–‡å…³ç³»
        # æ¯”å¦‚ä¸€ä¸ªå¤´å…³æ³¨é¢œè‰²ï¼Œå¦ä¸€ä¸ªå¤´å…³æ³¨å½¢çŠ¶
        self.multihead_attn = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=n_heads,
            batch_first=True
        )
        
        # å±‚å½’ä¸€åŒ–ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹
        self.norm = nn.LayerNorm(d_model)
        
        print(f"   ç‰¹å¾ç»´åº¦: {d_model}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
        print(f"   æ¯ä¸ªå¤´çš„ç»´åº¦: {self.head_dim}")
    
    def forward(self, text_features, image_features):
        """
        è·¨æ¨¡æ€æ³¨æ„åŠ›è®¡ç®—
        
        ðŸ”„ è®¡ç®—æµç¨‹ï¼š
        1. æ–‡å­—ç‰¹å¾ä½œä¸ºQueryï¼ˆé—®é¢˜ï¼‰
        2. å›¾ç‰‡ç‰¹å¾ä½œä¸ºKeyå’ŒValueï¼ˆç­”æ¡ˆï¼‰
        3. è®¡ç®—æ³¨æ„åŠ›æƒé‡
        4. æ ¹æ®æƒé‡èžåˆå›¾ç‰‡ä¿¡æ¯
        
        ðŸ“Š è¾“å…¥ï¼š
        - text_features: (B, seq_len, d_model) æ–‡å­—ç‰¹å¾
        - image_features: (B, num_patches, d_model) å›¾ç‰‡ç‰¹å¾
        
        ðŸ“Š è¾“å‡ºï¼š
        - attended_features: (B, seq_len, d_model) èžåˆåŽçš„ç‰¹å¾
        """
        # è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼šæ–‡å­—å…³æ³¨ç›¸å…³çš„å›¾ç‰‡åŒºåŸŸ
        # query=æ–‡å­—, key=å›¾ç‰‡, value=å›¾ç‰‡
        attended_features, attention_weights = self.multihead_attn(
            query=text_features,      # æ–‡å­—é—®ï¼š"æˆ‘éœ€è¦ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"
            key=image_features,       # å›¾ç‰‡ç­”ï¼š"æˆ‘æœ‰è¿™äº›ä¿¡æ¯å¯ä»¥æä¾›"
            value=image_features      # å›¾ç‰‡çš„å…·ä½“å†…å®¹
        )
        
        # æ®‹å·®è¿žæŽ¥ï¼šä¿ç•™åŽŸå§‹æ–‡å­—ä¿¡æ¯
        # è¿™æ ·æ—¢æœ‰æ–°çš„å›¾ç‰‡ä¿¡æ¯ï¼Œä¹Ÿä¸ä¸¢å¤±åŽŸæ¥çš„æ–‡å­—ä¿¡æ¯
        attended_features = self.norm(attended_features + text_features)
        
        return attended_features, attention_weights

class VLMTransformerBlock(nn.Module):
    """
    VLM Transformerå— - æ ¸å¿ƒçš„ç†è§£å’Œèžåˆå•å…ƒ
    
    ðŸ§  ä½œç”¨ï¼šæ·±åº¦ç†è§£å’Œèžåˆæ–‡å­—ã€å›¾ç‰‡ä¿¡æ¯
    ðŸ”„ å¤„ç†æµç¨‹ï¼š
    1. è‡ªæ³¨æ„åŠ›ï¼šæ–‡å­—å†…éƒ¨çš„å…³ç³»ç†è§£
    2. è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼šæ–‡å­—å…³æ³¨ç›¸å…³å›¾ç‰‡åŒºåŸŸ
    3. å‰é¦ˆç½‘ç»œï¼šè¿›ä¸€æ­¥å¤„ç†èžåˆä¿¡æ¯
    
    ðŸ’¡ è®¾è®¡ç†å¿µï¼š
    - æ¯ä¸ªå—éƒ½è®©æ¨¡åž‹çš„ç†è§£æ›´æ·±ä¸€å±‚
    - å¤šä¸ªå—å åŠ ï¼Œå½¢æˆæ·±åº¦ç†è§£èƒ½åŠ›
    """
    def __init__(self, d_model, n_heads):
        super().__init__()
        print(f"ðŸ§  åˆå§‹åŒ–VLM Transformerå—...")
        
        # è‡ªæ³¨æ„åŠ›ï¼šç†è§£æ–‡å­—åºåˆ—å†…éƒ¨çš„å…³ç³»
        # æ¯”å¦‚ç†è§£"çº¢è‰²çš„æ–¹å—"ä¸­"çº¢è‰²"ä¿®é¥°"æ–¹å—"
        self.self_attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=n_heads,
            batch_first=True
        )
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼šè®©æ–‡å­—å…³æ³¨ç›¸å…³çš„å›¾ç‰‡åŒºåŸŸ
        self.cross_attention = CrossModalAttention(d_model, n_heads)
        
        # å‰é¦ˆç½‘ç»œï¼šè¿›ä¸€æ­¥å¤„ç†èžåˆåŽçš„ä¿¡æ¯
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_model * 4),  # æ‰©å±•ç»´åº¦
            nn.ReLU(),                        # éžçº¿æ€§æ¿€æ´»
            nn.Linear(d_model * 4, d_model)   # åŽ‹ç¼©å›žåŽŸç»´åº¦
        )
        
        # å±‚å½’ä¸€åŒ–ï¼šç¨³å®šè®­ç»ƒï¼ŒåŠ é€Ÿæ”¶æ•›
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        print(f"   ç‰¹å¾ç»´åº¦: {d_model}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
    
    def forward(self, text_features, image_features):
        """
        VLMå—çš„å®Œæ•´å¤„ç†æµç¨‹
        
        ðŸ”„ ä¸‰æ­¥å¤„ç†ï¼š
        1. è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿žæŽ¥
        2. è·¨æ¨¡æ€æ³¨æ„åŠ› + æ®‹å·®è¿žæŽ¥  
        3. å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿žæŽ¥
        """
        # ç¬¬ä¸€æ­¥ï¼šè‡ªæ³¨æ„åŠ› - ç†è§£æ–‡å­—å†…éƒ¨å…³ç³»
        attn_output, _ = self.self_attention(text_features, text_features, text_features)
        text_features = self.norm1(text_features + attn_output)
        
        # ç¬¬äºŒæ­¥ï¼šè·¨æ¨¡æ€æ³¨æ„åŠ› - æ–‡å­—å…³æ³¨å›¾ç‰‡
        cross_attn_output, attention_weights = self.cross_attention(text_features, image_features)
        text_features = self.norm2(text_features + cross_attn_output)
        
        # ç¬¬ä¸‰æ­¥ï¼šå‰é¦ˆç½‘ç»œ - è¿›ä¸€æ­¥å¤„ç†
        ff_output = self.feed_forward(text_features)
        text_features = self.norm3(text_features + ff_output)
        
        return text_features, attention_weights

class SimpleVLM(nn.Module):
    """
    ç®€å•çš„è§†è§‰è¯­è¨€æ¨¡åž‹ - æ•´åˆæ‰€æœ‰ç»„ä»¶
    
    ðŸ¤– ä½œç”¨ï¼šèƒ½å¤Ÿçœ‹å›¾è¯´è¯çš„AIæ¨¡åž‹
    ðŸ—ï¸ æž¶æž„ï¼š
    1. è§†è§‰ç¼–ç å™¨ï¼šå¤„ç†å›¾ç‰‡
    2. æ–‡å­—åµŒå…¥ï¼šå¤„ç†æ–‡å­—
    3. VLMå—ï¼šæ·±åº¦èžåˆç†è§£
    4. è¾“å‡ºå±‚ï¼šç”Ÿæˆæ–‡å­—
    
    ðŸŽ¯ èƒ½åŠ›ï¼š
    - å›¾åƒæè¿°ï¼šçœ‹å›¾ç‰‡ï¼Œè¯´å‡ºå†…å®¹
    - è§†è§‰é—®ç­”ï¼šåŸºäºŽå›¾ç‰‡å›žç­”é—®é¢˜
    - å¤šæ¨¡æ€ç†è§£ï¼šåŒæ—¶ç†è§£å›¾ç‰‡å’Œæ–‡å­—
    """
    def __init__(self, vocab_size, d_model=128, n_heads=8, n_layers=6, max_seq_len=100):
        super().__init__()
        print(f"ðŸ¤– åˆå§‹åŒ–SimpleVLM...")
        
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        
        # æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
        print("   æ­£åœ¨åˆå§‹åŒ–å„ä¸ªç»„ä»¶...")
        
        # 1. è§†è§‰ç¼–ç å™¨ï¼šVLMçš„"çœ¼ç›"
        self.vision_encoder = SimpleVisionEncoder(d_model=d_model)
        
        # 2. æ–‡å­—åµŒå…¥ï¼šæŠŠæ–‡å­—è½¬æ¢æˆç‰¹å¾å‘é‡
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Parameter(torch.randn(1, max_seq_len, d_model))
        
        # 3. VLMå¤„ç†å—ï¼šæ ¸å¿ƒçš„ç†è§£å’Œèžåˆ
        self.vlm_blocks = nn.ModuleList([
            VLMTransformerBlock(d_model, n_heads) for _ in range(n_layers)
        ])
        
        # 4. è¾“å‡ºå±‚ï¼šæŠŠç‰¹å¾è½¬æ¢å›žæ–‡å­—
        self.output_projection = nn.Linear(d_model, vocab_size)
        
        # è®¡ç®—æ€»å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        print(f"   è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"   æ¨¡åž‹ç»´åº¦: {d_model}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
        print(f"   VLMå±‚æ•°: {n_layers}")
        print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
    
    def forward(self, image, text_tokens):
        """
        VLMçš„å®Œæ•´å‰å‘ä¼ æ’­
        
        ðŸ”„ å¤„ç†æµç¨‹ï¼š
        1. å›¾ç‰‡ â†’ è§†è§‰ç‰¹å¾
        2. æ–‡å­— â†’ æ–‡å­—ç‰¹å¾
        3. å¤šå±‚VLMå¤„ç† â†’ æ·±åº¦èžåˆ
        4. è¾“å‡ºå±‚ â†’ é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
        """
        B, seq_len = text_tokens.shape
        
        # ç¬¬ä¸€æ­¥ï¼šå¤„ç†å›¾ç‰‡ï¼Œæå–è§†è§‰ç‰¹å¾
        image_features = self.vision_encoder(image)  # (B, 196, d_model)
        
        # ç¬¬äºŒæ­¥ï¼šå¤„ç†æ–‡å­—ï¼Œè½¬æ¢ä¸ºç‰¹å¾å‘é‡
        text_features = self.token_embedding(text_tokens)  # (B, seq_len, d_model)
        
        # æ·»åŠ ä½ç½®ç¼–ç ï¼šå‘Šè¯‰æ¨¡åž‹æ¯ä¸ªè¯çš„ä½ç½®
        text_features = text_features + self.position_embedding[:, :seq_len, :]
        
        # ç¬¬ä¸‰æ­¥ï¼šå¤šå±‚VLMå¤„ç†ï¼Œé€æ­¥æ·±åŒ–ç†è§£
        attention_maps = []  # ä¿å­˜æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºŽåˆ†æž
        for vlm_block in self.vlm_blocks:
            text_features, attention_weights = vlm_block(text_features, image_features)
            attention_maps.append(attention_weights)
        
        # ç¬¬å››æ­¥ï¼šè¾“å‡ºå±‚ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚çŽ‡åˆ†å¸ƒ
        logits = self.output_projection(text_features)  # (B, seq_len, vocab_size)
        
        return logits, attention_maps
    
    def generate(self, image, tokenizer, prompt="", max_length=20, temperature=1.0):
        """
        ç”Ÿæˆå›¾ç‰‡æè¿°
        
        ðŸŽ¯ ç”Ÿæˆç­–ç•¥ï¼š
        1. ä»Žæç¤ºè¯å¼€å§‹ï¼ˆå¦‚æžœæœ‰ï¼‰
        2. é€ä¸ªé¢„æµ‹ä¸‹ä¸€ä¸ªè¯
        3. ç›´åˆ°è¾¾åˆ°æœ€å¤§é•¿åº¦æˆ–ç”Ÿæˆç»“æŸç¬¦
        
        ðŸŒ¡ï¸ æ¸©åº¦å‚æ•°ï¼š
        - ä½Žæ¸©åº¦(0.5)ï¼šç”Ÿæˆæ›´ç¡®å®šã€ä¿å®ˆçš„æ–‡æœ¬
        - é«˜æ¸©åº¦(1.5)ï¼šç”Ÿæˆæ›´éšæœºã€åˆ›æ–°çš„æ–‡æœ¬
        """
        self.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        
        with torch.no_grad():
            # åˆå§‹åŒ–ç”Ÿæˆåºåˆ—
            if prompt:
                generated = tokenizer.encode(prompt)
            else:
                generated = []
            
            # é€ä¸ªç”Ÿæˆè¯æ±‡
            for _ in range(max_length):
                # å‡†å¤‡è¾“å…¥
                if len(generated) == 0:
                    # å¦‚æžœæ²¡æœ‰åˆå§‹è¯ï¼Œç”¨ä¸€ä¸ªå ä½ç¬¦
                    current_tokens = torch.tensor([[0]], dtype=torch.long)
                else:
                    current_tokens = torch.tensor([generated[-self.max_seq_len:]], dtype=torch.long)
                
                # å‰å‘ä¼ æ’­
                logits, _ = self.forward(image, current_tokens)
                
                # èŽ·å–æœ€åŽä¸€ä¸ªä½ç½®çš„é¢„æµ‹
                next_token_logits = logits[0, -1, :] / temperature
                
                # åº”ç”¨softmaxèŽ·å–æ¦‚çŽ‡åˆ†å¸ƒ
                probs = F.softmax(next_token_logits, dim=-1)
                
                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                next_token = torch.multinomial(probs, 1).item()
                
                generated.append(next_token)
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢ç”Ÿæˆ
                if len(generated) > 1 and next_token == generated[0]:  # ç®€å•çš„åœæ­¢æ¡ä»¶
                    break
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        return tokenizer.decode(generated)

# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ¼”ç¤ºå’Œåˆ†æžå‡½æ•°
# ============================================================================

def create_demo_image():
    """
    åˆ›å»ºæ¼”ç¤ºç”¨çš„çº¢è‰²æ–¹å—å›¾ç‰‡
    
    ðŸŽ¨ å›¾ç‰‡å†…å®¹ï¼š224Ã—224çš„å›¾ç‰‡ï¼Œä¸­å¤®æœ‰ä¸€ä¸ªçº¢è‰²æ–¹å—
    ðŸ“Š æ ¼å¼ï¼šRGBä¸‰é€šé“ï¼Œå€¼èŒƒå›´0-1
    """
    print("ðŸŽ¨ åˆ›å»ºæ¼”ç¤ºå›¾ç‰‡...")
    
    # åˆ›å»ºç©ºç™½å›¾ç‰‡ï¼ˆç°è‰²èƒŒæ™¯ï¼‰
    image = torch.ones(3, 224, 224) * 0.5  # ç°è‰²èƒŒæ™¯
    
    # åœ¨ä¸­å¤®ç”»çº¢è‰²æ–¹å—
    center = 112
    size = 40
    
    # çº¢è‰²æ–¹å—åŒºåŸŸ
    image[0, center-size:center+size, center-size:center+size] = 0.8  # çº¢è‰²é€šé“
    image[1, center-size:center+size, center-size:center+size] = 0.1  # ç»¿è‰²é€šé“
    image[2, center-size:center+size, center-size:center+size] = 0.1  # è“è‰²é€šé“
    
    print("   âœ… åˆ›å»ºäº†224Ã—224çš„å›¾ç‰‡ï¼Œä¸­å¤®æœ‰çº¢è‰²æ–¹å—")
    return image.unsqueeze(0)  # æ·»åŠ batchç»´åº¦

def analyze_attention_patterns(attention_maps, tokenizer, text_tokens, step_name=""):
    """
    åˆ†æžæ³¨æ„åŠ›æ¨¡å¼ï¼Œç†è§£æ¨¡åž‹åœ¨"çœ‹"ä»€ä¹ˆ
    
    ðŸ” åˆ†æžå†…å®¹ï¼š
    - æ¯ä¸ªè¯å…³æ³¨å›¾ç‰‡çš„å“ªäº›åŒºåŸŸ
    - æ³¨æ„åŠ›çš„é›†ä¸­ç¨‹åº¦
    - ä¸åŒå±‚çš„æ³¨æ„åŠ›å˜åŒ–
    """
    print(f"\nðŸ” {step_name}æ³¨æ„åŠ›æ¨¡å¼åˆ†æž:")
    
    # åˆ†æžæœ€åŽä¸€å±‚çš„æ³¨æ„åŠ›
    if len(attention_maps) > 0:
        last_attention = attention_maps[-1][0]  # (seq_len, num_patches)
        
        for i, token_id in enumerate(text_tokens[0][:min(5, len(text_tokens[0]))]):
            if i < last_attention.shape[0]:
                word = tokenizer.decode([token_id.item()])
                attention_weights = last_attention[i]
                
                # æ‰¾åˆ°æ³¨æ„åŠ›æœ€é«˜çš„å›¾ç‰‡åŒºåŸŸ
                max_attention_patch = torch.argmax(attention_weights).item()
                max_attention_value = torch.max(attention_weights).item()
                
                # è®¡ç®—æ³¨æ„åŠ›çš„é›†ä¸­ç¨‹åº¦ï¼ˆç†µï¼‰
                probs = F.softmax(attention_weights, dim=0)
                entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()
                
                print(f"   '{word}': æœ€å…³æ³¨åŒºåŸŸ{max_attention_patch} (æƒé‡:{max_attention_value:.3f}, åˆ†æ•£åº¦:{entropy:.2f})")

def train_vlm_step_by_step():
    """
    ä¸€æ­¥æ­¥è®­ç»ƒVLMï¼Œå±•ç¤ºå®Œæ•´çš„å­¦ä¹ è¿‡ç¨‹
    
    ðŸŽ“ è®­ç»ƒé˜¶æ®µï¼š
    1. æ•°æ®å‡†å¤‡
    2. æ¨¡åž‹åˆå§‹åŒ–  
    3. è®­ç»ƒå¾ªçŽ¯
    4. ç”Ÿæˆæµ‹è¯•
    5. æ³¨æ„åŠ›åˆ†æž
    """
    print("\nðŸŽ“ å¼€å§‹VLMè®­ç»ƒæ¼”ç¤º")
    print("=" * 50)
    
    # ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®
    print("\nðŸ“š ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®")
    training_text = "è¿™æ˜¯ä¸€ä¸ªçº¢è‰²çš„æ–¹å—ã€‚å›¾åƒä¸­å¤®æœ‰ä¸€ä¸ªçº¢è‰²ç‰©ä½“ã€‚çº¢è‰²æ–¹å—ä½äºŽå›¾åƒä¸­å¿ƒã€‚è¿™ä¸ªç‰©ä½“æ˜¯çº¢è‰²çš„ã€‚æ–¹å—æ˜¯çº¢è‰²çš„ã€‚çº¢è‰²çš„æ–¹å—åœ¨å›¾åƒä¸­ã€‚"
    
    tokenizer = SimpleTokenizer(training_text)
    text_tokens = torch.tensor([tokenizer.encode(training_text)], dtype=torch.long)
    
    print(f"   è®­ç»ƒæ–‡æœ¬: '{training_text[:30]}...'")
    print(f"   æ–‡æœ¬é•¿åº¦: {len(training_text)} å­—ç¬¦")
    print(f"   Tokenæ•°é‡: {text_tokens.shape[1]}")
    
    # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºæ¨¡åž‹
    print("\nðŸ¤– ç¬¬äºŒæ­¥ï¼šåˆ›å»ºVLMæ¨¡åž‹")
    model = SimpleVLM(
        vocab_size=tokenizer.vocab_size,
        d_model=128,
        n_heads=8,
        n_layers=4
    )
    
    # ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºå›¾ç‰‡å’Œä¼˜åŒ–å™¨
    print("\nðŸŽ¨ ç¬¬ä¸‰æ­¥ï¼šå‡†å¤‡è®­ç»ƒçŽ¯å¢ƒ")
    image = create_demo_image()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    # ç¬¬å››æ­¥ï¼šè®­ç»ƒå¾ªçŽ¯
    print("\nðŸ‹ï¸ ç¬¬å››æ­¥ï¼šå¼€å§‹è®­ç»ƒ")
    print("   è®­ç»ƒè¿›åº¦:")
    
    model.train()
    for epoch in range(41):
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        input_tokens = text_tokens[:, :-1]  # è¾“å…¥åºåˆ—
        target_tokens = text_tokens[:, 1:]  # ç›®æ ‡åºåˆ—ï¼ˆä¸‹ä¸€ä¸ªè¯ï¼‰
        
        logits, attention_maps = model(image, input_tokens)
        
        # è®¡ç®—æŸå¤±
        loss = criterion(logits.reshape(-1, tokenizer.vocab_size), target_tokens.reshape(-1))
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # æ¯10ä¸ªepochæ˜¾ç¤ºè¿›åº¦å’Œåˆ†æž
        if epoch % 10 == 0:
            print(f"   Epoch {epoch:2d}: Loss = {loss.item():.4f}")
            
            # åˆ†æžæ³¨æ„åŠ›æ¨¡å¼çš„å˜åŒ–
            if epoch in [0, 20, 40]:
                analyze_attention_patterns(
                    attention_maps, tokenizer, input_tokens, 
                    f"Epoch {epoch} "
                )
    
    # ç¬¬äº”æ­¥ï¼šæµ‹è¯•ç”Ÿæˆ
    print("\nðŸŽ¯ ç¬¬äº”æ­¥ï¼šæµ‹è¯•æ–‡æœ¬ç”Ÿæˆ")
    model.eval()
    
    for temp in [0.5, 1.0, 1.5]:
        generated_text = model.generate(image, tokenizer, max_length=10, temperature=temp)
        print(f"   æ¸©åº¦{temp}: '{generated_text}'")
    
    print("\nâœ… è®­ç»ƒæ¼”ç¤ºå®Œæˆï¼")
    return model, tokenizer, image

def demonstrate_components():
    """
    æ¼”ç¤ºVLMå„ä¸ªç»„ä»¶çš„å·¥ä½œåŽŸç†
    
    ðŸ”§ æ¼”ç¤ºå†…å®¹ï¼š
    1. åˆ†è¯å™¨çš„ç¼–ç è§£ç 
    2. è§†è§‰ç¼–ç å™¨çš„å›¾ç‰‡å¤„ç†
    3. è·¨æ¨¡æ€æ³¨æ„åŠ›çš„è®¡ç®—
    4. å®Œæ•´æ¨¡åž‹çš„å‰å‘ä¼ æ’­
    """
    print("\nðŸ”§ VLMç»„ä»¶å·¥ä½œåŽŸç†æ¼”ç¤º")
    print("=" * 40)
    
    # æ¼”ç¤º1ï¼šåˆ†è¯å™¨
    print("\nðŸ“ æ¼”ç¤º1ï¼šåˆ†è¯å™¨å·¥ä½œåŽŸç†")
    text = "çº¢è‰²æ–¹å—"
    tokenizer = SimpleTokenizer("çº¢è‰²æ–¹å—åœ¨å›¾åƒä¸­")
    encoded = tokenizer.encode(text)
    decoded = tokenizer.decode(encoded)
    
    print(f"   åŽŸæ–‡: '{text}'")
    print(f"   ç¼–ç : {encoded}")
    print(f"   è§£ç : '{decoded}'")
    
    # æ¼”ç¤º2ï¼šè§†è§‰ç¼–ç å™¨
    print("\nðŸ‘ï¸ æ¼”ç¤º2ï¼šè§†è§‰ç¼–ç å™¨å·¥ä½œåŽŸç†")
    vision_encoder = SimpleVisionEncoder(d_model=128)
    image = create_demo_image()
    
    print(f"   è¾“å…¥å›¾ç‰‡å½¢çŠ¶: {image.shape}")
    
    with torch.no_grad():
        image_features = vision_encoder(image)
        print(f"   è¾“å‡ºç‰¹å¾å½¢çŠ¶: {image_features.shape}")
        print(f"   åŽ‹ç¼©æ¯”ä¾‹: {(3*224*224)/(image_features.shape[1]*image_features.shape[2]):.1f}:1")
    
    # æ¼”ç¤º3ï¼šè·¨æ¨¡æ€æ³¨æ„åŠ›
    print("\nðŸ”— æ¼”ç¤º3ï¼šè·¨æ¨¡æ€æ³¨æ„åŠ›å·¥ä½œåŽŸç†")
    cross_attention = CrossModalAttention(d_model=128, n_heads=8)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡å­—ç‰¹å¾
    text_features = torch.randn(1, 5, 128)  # 5ä¸ªè¯çš„ç‰¹å¾
    
    with torch.no_grad():
        attended_features, attention_weights = cross_attention(text_features, image_features)
        
        print(f"   æ–‡å­—ç‰¹å¾å½¢çŠ¶: {text_features.shape}")
        print(f"   å›¾ç‰‡ç‰¹å¾å½¢çŠ¶: {image_features.shape}")
        print(f"   æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
        print(f"   èžåˆåŽç‰¹å¾å½¢çŠ¶: {attended_features.shape}")

def interactive_demo():
    """
    äº¤äº’å¼æ¼”ç¤º - è®©ç”¨æˆ·ä½“éªŒVLMçš„å„ä¸ªåŠŸèƒ½
    """
    print("\nðŸŽ® VLMäº¤äº’å¼æ¼”ç¤º")
    print("=" * 40)
    print("é€‰æ‹©ä½ æƒ³ä½“éªŒçš„åŠŸèƒ½:")
    print("1. å®Œæ•´è®­ç»ƒæ¼”ç¤º")
    print("2. ç»„ä»¶å·¥ä½œåŽŸç†")
    print("3. æ³¨æ„åŠ›å¯è§†åŒ–")
    print("4. æ–‡æœ¬ç”Ÿæˆæµ‹è¯•")
    print("5. é€€å‡º")
    
    while True:
        try:
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
            
            if choice == '1':
                train_vlm_step_by_step()
            elif choice == '2':
                demonstrate_components()
            elif choice == '3':
                print("ðŸ” æ³¨æ„åŠ›å¯è§†åŒ–æ¼”ç¤º")
                model, tokenizer, image = train_vlm_step_by_step()
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„æ³¨æ„åŠ›å¯è§†åŒ–
            elif choice == '4':
                print("ðŸŽ¯ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•")
                model, tokenizer, image = train_vlm_step_by_step()
            elif choice == '5':
                print("ðŸ‘‹ æ„Ÿè°¢ä½¿ç”¨VLMæ¼”ç¤ºç¨‹åºï¼")
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-5")
                
        except KeyboardInterrupt:
            print("\nðŸ‘‹ ç¨‹åºå·²é€€å‡º")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

# ============================================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================================

def main():
    """
    ä¸»ç¨‹åº - è¿è¡Œå®Œæ•´çš„VLMæ¼”ç¤º
    """
    print("\nðŸŽ‰ è¿™æ˜¯é›¶åŸºç¡€å…¥é—¨åšå®¢çš„é…å¥—æ¼”ç¤ºç¨‹åº")
    print("çŽ°åœ¨ä½ å¯ä»¥:")
    print("âœ… ç†è§£çœŸå®žVLMçš„å·¥ä½œåŽŸç†")
    print("âœ… çœ‹åˆ°ä¸ŽæŠ€æœ¯åšå®¢ä¸€è‡´çš„å®žçŽ°")
    print("âœ… ä½“éªŒå®Œæ•´çš„è®­ç»ƒå’Œç”Ÿæˆè¿‡ç¨‹")
    print("âœ… åˆ†æžæ¨¡åž‹çš„æ³¨æ„åŠ›æœºåˆ¶")
    
    # è¿è¡Œå®Œæ•´æ¼”ç¤º
    model, tokenizer, image = train_vlm_step_by_step()
    
    print("\nðŸŽ“ å­¦ä¹ æ€»ç»“:")
    print("é€šè¿‡è¿™ä¸ªæ¼”ç¤ºï¼Œä½ å­¦ä¼šäº†:")
    print("â€¢ VLMçš„å®Œæ•´æž¶æž„å’Œå®žçŽ°")
    print("â€¢ æ¯ä¸ªç»„ä»¶çš„å…·ä½“ä½œç”¨")
    print("â€¢ è®­ç»ƒè¿‡ç¨‹çš„è¯¦ç»†åˆ†æž")
    print("â€¢ æ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŽŸç†")
    print("â€¢ å¦‚ä½•ç”Ÿæˆå›¾ç‰‡æè¿°")
    
    print("\nðŸ“š ä»£ç ä¸€è‡´æ€§ä¿è¯:")
    print("â€¢ ä¸ŽæŠ€æœ¯åšå®¢ä½¿ç”¨ç›¸åŒçš„æž¶æž„")
    print("â€¢ ç›¸åŒçš„ç»„ä»¶è®¾è®¡å’Œå®žçŽ°")
    print("â€¢ ä¸€è‡´çš„å‚æ•°è®¾ç½®å’Œè®­ç»ƒæ–¹å¼")
    print("â€¢ åªæ˜¯å¢žåŠ äº†æ›´è¯¦ç»†çš„è§£é‡Š")

if __name__ == "__main__":
    # æ£€æŸ¥PyTorchæ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print("ðŸ’¡ æç¤ºï¼šæœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè¿è¡Œï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    
    # å¯ä»¥é€‰æ‹©è¿è¡Œäº¤äº’å¼æ¼”ç¤ºæˆ–å®Œæ•´æ¼”ç¤º
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--interactive":
        interactive_demo()
    else:
        main()

